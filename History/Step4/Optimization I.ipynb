{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "answering-compilation",
   "metadata": {},
   "source": [
    "## Blog목적\n",
    "\n",
    "이번 블로그에서는 최적화에 대해서 알아보겠습니다.\n",
    "\n",
    "Linear classifier에서 w를 최적으로 찾는 방식에 대해서 알아보도록 하겠습니다.\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/963eab96-e940-416d-9db8-2ccda9cefdd0/image.png)\n",
    "\n",
    "\n",
    "이미지 출처: [링크텍스트](https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture04.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "## Optimization\n",
    "\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/74037586-3c50-4a57-a67e-0cc5b9d57b4c/image.png)\n",
    "\n",
    "이미지 출처: [링크텍스트](http://www.aistudy.com/math/optimization.htmhttp://www.aistudy.com/math/optimization.htm)\n",
    "\n",
    "\n",
    "- Optimization은 최적의 w를 찾는 것이 아닌 loss를 최소화하는 w를 찾는 것입니다.\n",
    "\n",
    "\n",
    "- 목적함수의 함수값(최댓값, 최솟값)을 최적화 시키는 파라미터를 찾는 것입니다.\n",
    "\n",
    "ex) 카메라 초점거리 찾기. 사람의 관절 움직임 관찰하는 모델링,...등등\n",
    "\n",
    "\n",
    "### How to change W\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Random Search\n",
    "\n",
    "- 기준없이 무작위하게 W를 변경하는 것입니다.\n",
    "\n",
    "- Loss의 개선 여부로 W를 갱신합니다.\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/831a9bd1-c7a8-4132-afc3-8768a9bbd61d/image.png)\n",
    "\n",
    "- CIFAR 10으로 실험했을 때 15.5%라는 좋지 않은 결과를 냅니다.\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/12efbcff-d362-40e1-a447-6136c32afade/image.png)\n",
    "\n",
    "\n",
    "- 즉, Bad Idea입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Flow the slope\n",
    "\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/7d7774ac-7d0a-4e37-85a6-e1e1e9ca757b/image.png)\n",
    "\n",
    "\n",
    "- 손실함수를 따라서 최적의 W를 찾는 방법입니다.\n",
    "\n",
    "- 손실함수가 1차원이면 도함수를 이용하면 됩니다.\n",
    "\n",
    "- 손실함수가 1차원이 아니면 gradient를 이용합니다.\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/f62d7985-d9fd-4748-9ab1-af12c6555bb0/image.png)\n",
    "\n",
    "- gradient는 각 차원을 따르는 백터입니다.\n",
    "\n",
    "- slope의 방향은 각 gradient의 방향을 내적한 것입니다.\n",
    "\n",
    "- steepest descent의 방향은 negative gradient입니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Numeric Gradient\n",
    "\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/5f804574-2fb7-413c-851d-522f2e24db7d/image.png)\n",
    "\n",
    "\n",
    "- Gradient를 구하는 방법입니다.\n",
    "\n",
    "- 벡터 W에서 요소를 1개씩 번갈아서 각 요소에 대응하는 gradient를 구할 수 있게 됩니다.\n",
    "\n",
    "- 계산 횟수가 W의 크기에 비례하기에 W가 크면 좋지 않은 성능을 보입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Analytic Gradient\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/1ce293d1-d47d-4541-a8d1-d862fe59b9de/image.png)\n",
    "\n",
    "- 미분을 이용하여 gradient를 구합니다.\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/cbb319cc-6c07-42ac-8a67-4acb2761c317/image.png)\n",
    "\n",
    "- 정확하지만 복잡하여 구현 실수가 일어납니다.\n",
    "\n",
    "#### 총 정리\n",
    "\n",
    "\n",
    "- Numeric gradient: 근접성, 느림, 쓰기 편함\n",
    "\n",
    "- Analytic gradient: 정확함, 빠름, error-prone\n",
    "\n",
    "- Analytic gradient개발 시 구현의 실수가 있는지를 확인하기 위해 Numeric Gradient를 사용하여 gradient check를 합니다. \n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/f8090f77-a784-450e-adf6-a3a8055e376c/image.png)\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/61200485-517f-4beb-b5e0-b73435d3bfba/image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-detail",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
