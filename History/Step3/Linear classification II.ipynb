{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "appointed-antique",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    " \n",
    " ![](https://images.velog.io/images/qsdcfd/post/cf53d72e-85e5-4cc1-89a8-319529b9091c/image.png)\n",
    "\n",
    "Loss function은 현재의 클래스를 분류하는 분류기가 좋은 성능을 보여주는지를 알려주는 척도로, 출력값을 loss라 하고 loss와 classifier은 반비례합니다.\n",
    "\n",
    "왜냐하면, loss가 낮을 수록 classifier의 성능이 좋은 것을 의미합니다.\n",
    "\n",
    "\n",
    "물론,  Loss에 마이너스를 붙이면 loss와 classifier은 비례합니다.\n",
    "\n",
    "*loss는 항상 작아지는 방향으로 W가 학습이 됩니다.*\n",
    " \n",
    " \n",
    " ### Multiclass SVM Loss\n",
    " \n",
    " ![](https://images.velog.io/images/qsdcfd/post/76c220fb-b6c6-4c85-905a-2639be593c06/image.png)\n",
    "\n",
    "SVM Loss, Loss가 다른 class의 class score와 정답 클래스의 score의 차가 margin보다 크면 0, 아니면 차이에 margin을 더합니다\n",
    "\n",
    "*보통 margin =1\n",
    "\n",
    "아래의 사진은 Multiclass SVM Loss의 예시입니다.\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/68c28a19-4e4f-456e-82b0-72b82dcb10ab/image.png)\n",
    "\n",
    "추가적으로, SVM Loss의 특징에 대해서 설명합니다.\n",
    "\n",
    ">\n",
    "\n",
    "- margin이 크면, class score가 바뀌더라도 loss가 0이면 계속 0이 됩니다.(by max())\n",
    "\n",
    "\n",
    "- max연산을 사용하기에 최솟값은 0이고 최댓값은 무한합니다.\n",
    "\n",
    "- score가 무작위로 나와도 가우시안 분포를 따르게 되면 각 score의 차의 합은 0에 근접하게 됩니다. 그러므로, Loss = (C-1)* margin이라는 식이 나옵니다.(*C= 클래스 수)\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/0f51c4ea-11ab-48bf-88c8-339a8c452f65/image.png)\n",
    "\n",
    "이미지 출처:[링크텍스트](https://brunch.co.kr/@linecard/541)\n",
    "\n",
    "- Sum대신 mean을 사용하더라도 loss는 달라지겠지만 결과는 동일합니다.\n",
    "\n",
    "- Sum대신 제곱을 취하면 선형성을 잃기에 loss도 달라지고 결과도 달라집니다.\n",
    "\n",
    "- W는 유일한 것이 아니고 여러 개입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    " ### Regularization\n",
    " \n",
    "간단하게 Regularization을 설명해보면, 정의는 모델의 오버피팅을 방지하는 방법입니다.\n",
    "\n",
    "Linear classifier에서 사용을 할 때는 data loss + regularization(with lambda:hyperparameter)을 추가하여 사용을 합니다.\n",
    "\n",
    "다양한 regularization중 linear classifier에서는 주로 Dropout, batch normalization이 사용됩니다.\n",
    "\n",
    "![](https://images.velog.io/images/qsdcfd/post/d5efe7b8-4127-4731-a585-ccb3af9e5a6b/image.png)\n",
    "\n",
    "\n",
    " 실제로는 overfitting뿐만 아니라 다양한 효과가 있고 이제 그것들을 알아봅니다.\n",
    " \n",
    " <br>\n",
    "\n",
    " #### Expressing Preferences\n",
    " \n",
    " ![](https://images.velog.io/images/qsdcfd/post/3d59c6ce-f584-4c52-8a73-ef0401837001/image.png)\n",
    " \n",
    " - L1 regularization은 각각의 feature들이 서로 연관이 되기에 특정 feature를 고르기 어려운 경우에 사용이되고, 그 결과 하나의 feature에 weight가 몰립니다.\n",
    " \n",
    "\n",
    " - L2 regularization은 각각의 feature들이 서로 연관이 되기에 특정 feature를 고르기 어려운 경우에 사용이되고, 그 결과 모든 feature의 weight가 균등합니다.\n",
    " \n",
    " \n",
    "\n",
    " \n",
    " \n",
    " <br>\n",
    "\n",
    " #### Prefer Simpler Models\n",
    " \n",
    " ![](https://images.velog.io/images/qsdcfd/post/f02c2891-b156-4c6b-88e0-3ad16ac4b57b/image.png)\n",
    " \n",
    " \n",
    " 그림을 설명하면, f1은 학습 데이터(파란색)에 적합한 모델이고 f2는 그렇지 않습니다.\n",
    " \n",
    " 그렇기에 학습이 되지 않은 데이터(회색)이 들어오면 f2가 f1보다 대응을 잘할겁니다.\n",
    " \n",
    " 실제 데이터의 수가 학습 데이터(파란색) > 예측 데이터(회색)인 경우는 많지 않으므로 \n",
    " 학습 데이터에 적합한 모델(f1)보다 여러 데이터에 유연한 대체가 가능한 모델(f2)가 필요합니다.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " <br>\n",
    "\n",
    " ### Cross-Entropy Loss(Multinomial Logistic Regression, 두 가지 확률 비교)\n",
    " \n",
    " ![](https://images.velog.io/images/qsdcfd/post/cf7bcdaf-6c2b-4c94-8a52-1a49e129621d/image.png)\n",
    " \n",
    " 이 Loss는 raw classifier score를 확률로 활용하기 위한 방식으로 Softmax function을 사용합니다.\n",
    " \n",
    " Softmax는 일반적인 max함수처럼 최댓값이 1 혹은 0으로 극단적이지 않는 함수입니다.\n",
    " \n",
    " \n",
    " ![](https://images.velog.io/images/qsdcfd/post/b5a1d3c5-3e39-4f13-81fc-f3324ce87485/image.png)\n",
    "*process of raw classifier score to get probabilities\n",
    " \n",
    " \n",
    "추가적으로, Cross-Entropy Loss의 특징에 대해 설명해드립니다.\n",
    "\n",
    "- min loss: 0에 수렴, max loss: + inifinity\n",
    "\n",
    "- score가 가우시안 분포를 따르면 log(C)입니다.\n",
    "\n",
    "\n",
    " ### SVM Loss 와 Cross-Entropy Loss 차이점 정리\n",
    " \n",
    " \n",
    " \n",
    " ![](https://images.velog.io/images/qsdcfd/post/b9469cbe-142a-407b-ad4a-d05f278a27dd/image.png)\n",
    " \n",
    " \n",
    "- SVM Loss는 score가 margin 1이상이기에 0입니다.\n",
    "\n",
    "- Cross-Entropy는 0보다 큽니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- Score의 변동 시, SVM Loss는 margin과 score차이가 존재하므로 없습니다.\n",
    "\n",
    "- Score의 변동 시, Cross-entropy는 변할 겁니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- correct class score * 2, Cross- Entropy loss는 감소합니다.\n",
    "\n",
    "- correct class score * 2, SVM Loss는 Margin과 score의 차이가 점점 커지므로 0입니다.(max함수)\n",
    " \n",
    " \n",
    " 이제, Linear classifier이 끝났고 다음에는 Optimization블로그로 만나뵙겠습니다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-response",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
